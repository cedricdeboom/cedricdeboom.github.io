---
layout: post
title: "Wikipedia word2vec models"
excerpt: "I have just released soms word2vec models trained on Wikipedia"
categories: blog
tags: [data, word2vec, wikipedia]
comments: true
share: true
image:
  feature: word2vec-header.jpg
  credit: CodeProject
---

I have just released some word2vec models that I have trained on Wikipedia for my research on language models. There is a model that contains 400-dimensional high-quality embeddings for around 2.8M English words, and I also have 100-dimensional embeddings for 850K Dutch words.

The models are available at our research lab's new [_Datashare_](http://http://193.190.127.235/){:target="_blank"} website. It just launched today, but I hope to vastly extend the content over there in the coming months and years. If you use the embeddings in your research or project, please make a reference to our papers!